# 活动数据抓取说明

## 抓取流程

### 自动抓取（推荐）
GitHub Actions 会自动执行以下流程：

1. **清空数据**: 清空上次抓取的活动数据 JSON 文件
2. **重新抓取**: 从官方网站抓取最新活动数据
3. **数据处理**: 清洗、验证和格式化数据
4. **自动提交**: 将更新的数据提交到仓库
5. **触发构建**: 自动构建和部署网站

### 手动抓取
```bash
# 方式1: 清空并重新抓取（推荐）
npm run scrape:fresh

# 方式2: 分步执行
npm run clear:events    # 清空数据
npm run scrape:events   # 抓取数据

# 方式3: 强制完整抓取
npm run scrape:events:force
```

## 数据文件说明

### 清空的文件
抓取前会清空以下 JSON 数据文件：
- `src/data/events/processed-events.json` - 处理后的事件数据
- `src/data/events/city-mappings.json` - 城市映射关系
- `src/data/events/event-stats.json` - 事件统计信息
- `data/events/events.json` - 原始事件数据（如果存在）

### 保留的文件
以下文件不会被清空：
- 所有图片文件 (`public/images/`)
- 城市配置 (`src/data/cities.json`)
- 翻译文件 (`src/data/translations/`)
- 其他配置文件

## 抓取时机

### 自动触发
- **定时抓取**: 每天早上 8 点自动执行
- **代码推送**: 推送到 main 分支时执行
- **手动触发**: 在 GitHub Actions 页面手动运行

### 抓取模式
- **标准模式**: 正常的数据抓取
- **强制模式**: 强制重新抓取所有数据（忽略缓存）

## 错误处理

### 抓取失败
如果抓取失败，系统会：
1. 创建空的数据文件防止构建失败
2. 记录失败状态到统计文件
3. 继续执行构建流程
4. 不会阻止网站部署

### 数据验证
抓取完成后会验证：
- JSON 格式正确性
- 数据文件完整性
- 事件数量统计

## 监控和调试

### 查看抓取日志
1. 进入 GitHub 仓库
2. 点击 "Actions" 标签
3. 选择最近的工作流运行
4. 查看 "Scrape event data" 步骤的日志

### 本地调试
```bash
# 查看抓取脚本帮助
node scripts/run-scraper.cjs --help

# 启用详细日志
DEBUG=1 npm run scrape:events

# 查看数据处理日志
node scripts/process-events.js --verbose
```

## 数据流程图

```
清空上次数据
     ↓
从官网抓取原始数据
     ↓
数据清洗和验证
     ↓
城市映射处理
     ↓
生成统计信息
     ↓
保存处理后的数据
     ↓
提交到 Git 仓库
     ↓
触发网站构建
```

## 注意事项

1. **数据清空**: 每次抓取前都会清空上次的数据，确保数据新鲜度
2. **图片保留**: 图片文件不会被删除，避免重复下载
3. **增量更新**: Git 只会提交有变化的数据文件
4. **构建兼容**: 即使抓取失败，网站仍可正常构建和访问
5. **时区处理**: 所有时间戳使用 UTC 时区

## 常见问题

**Q: 为什么每次都要清空数据？**
A: 确保获取最新、完整的数据，避免旧数据残留影响。

**Q: 抓取失败会影响网站吗？**
A: 不会，系统会创建空数据文件，网站可正常访问。

**Q: 如何查看抓取到多少数据？**
A: 查看 GitHub Actions 日志或 `event-stats.json` 文件。

**Q: 可以跳过某些城市的抓取吗？**
A: 可以修改抓取脚本的配置文件来控制抓取范围。